#!/usr/bin/env python

"""
Antelope Datascope output to JSON files. 
Summary & individual created via cron once a day

@package  Datascope
@author   Rob Newman <robertlnewman@gmail.com> 858.822.1333
@version  2.0
@license  MIT style license
@modified 2011-07-18
@notes    1. Rewrite to only have dbpointers open for a short while - Datascope 
          does not like realtime dbs being open when edits can happen (dbpointers
          all become invalid and the script crashes)
          2. Datascope does not correctly free up memory. Triage this by forcing
          memory cleanup with datascope.dbfree() and datascope.dbclose(). This goes
          against Pythonic Principles (Python has dynamic garbage collection) but
          until underlying Datascope memory leaks are fixed, this is a stop-gap.
"""

import sys
import os
import json
import string
import tempfile
import re
import gzip
# Load datascope functions
sys.path.append(os.environ['ANTELOPE'] + '/local/data/python/antelope')
import datascope
import orb
from stock import pfupdate, pfget, epoch2str, epoch, str2epoch, strtime, yearday
from optparse import OptionParser
from time import time, gmtime, strftime

subtype_list = ['all', 'decom', 'adopt', 'active']
station_dict = { 'decom':{}, 'adopt':{},'active':{} }
core_fields = ["snet", "vnet", "lat", "lon", "staname", "time"]
active_fields = core_fields + ["commtype", "provider", "insname", "elev", "equip_install", "cert_time"]
decom_fields = core_fields + ["decert_time", "insname", "endtime", "cert_time"]
adopt_fields = core_fields + ["decert_time", "newsnet", "newsta", "atype", "auth"]
# For station detail pages (individual station JSON files)
detail_dbmaster_fields = core_fields + ["sta", "ondate", "offdate", "elev", "endtime", "equip_install", "equip_remove", "cert_time", "decert_time", "commtype", "provider", "power", "dutycycle", "insname"]
detail_dbmaster_adopt_fields  = adopt_fields + ["sta", "ondate", "offdate", "elev", "endtime", "equip_install", "equip_remove", "cert_time", "commtype", "provider", "power", "dutycycle", "insname"]
detail_inst_hist_fields = ["insname", "instype", "ssident", "chan", "hang", "vang", "sitechan.ondate", "sitechan.offdate", "gtype", "idtag"]
detail_deployment_hist_fields = ["time", "endtime", "vnet", "cert_time", "decert_time"]
detail_comms_hist_fields = ["time", "endtime", "commtype", "provider", "power", "dutycycle"]
detail_baler_fields = ["model", "firm", "nreboot", "last_reboot", "ssident"]
detail_dlevents = ["dlevtype", "dlcomment"]

usage = "Usage: %prog [options]"
parser = OptionParser(usage=usage)
parser.add_option("-v", action="store_true", dest="verbose", help="verbose output", default=False)
parser.add_option("-x", action="store_true", dest="debug", help="debug output", default=False)
parser.add_option("-t", "--type", action="store", type="string", dest="subtype", help="type of station to process", default=False)
parser.add_option("-z", action="store_true", dest="zipper", help="create a gzipped version of the file", default=False)
(options, args) = parser.parse_args()
if options.verbose:
    verbose = True
else:
    verbose = False
if options.debug:
    debug = True
else:
    debug = False
if options.zipper:
    zipper = True
else:
    zipper = False
if options.subtype:
    subtype = options.subtype
    if subtype not in subtype_list:
        spacer = ', '
        print "- Subtype %s not recognized. Please do not define it or use either: %s" % (subtype, spacer.join(subtype_list))
        exit()
else:
    subtype = False

def nfs_test(db):
    """Test that the disk
    mount point is visible
    """
    if not os.path.isfile(db):
        print " - Error: Cannot read the dbmaster file (%s). NFS problems? Permissions problems? Check the file exists..." % db
        exit()
    return True

def zip_me_up(myfile):
    """Create a gzipped 
    JSON file
    """
    fzip_in = open(myfile, 'rb')
    try:
        fzip_out = gzip.open('%s.gz' % myfile, 'wb' )
    except IOError,e:
        print "Error: %s:%s" % (IOError, e)
    else:
        fzip_out.writelines(fzip_in)
        fzip_out.close()
        fzip_in.close()
        return True

class ParseOrb:
    """Methods for grabbing the
    useful packet values out
    of the specified orb
    """
    def __init__(self, alerts):
        """Default args"""
        self.alerts = alerts

    def orb_interaction(self, orbptr, selection_string=False):
        """Open & select orb
        """
        if verbose:
            print " - Orb (%s) operations" % orbptr
        try:
            myorb = orb.Orb(orbptr, 'r')
        except Exception, e:
            print "  - Cannot open the orb %s. Caught exception: (%s)" % (orbptr, e)
            return False
        else:
            if myorb.select(selection_string) < 1:
                print "  - Problem with the orb select functionality!"
            else:
                if verbose:
                    print "  - Number of sources selected: %d" % myorb.select(selection_string)
                when, sources = myorb.sources()
                orb_dict = self.parse_orb_sources(sources)
            myorb.close()
        return orb_dict

    def parse_orb_sources(self, sources):
        """Parse the sources
        and return a dictionary
        """
        source_dict = {}
        for s in sources:
            srcname = s['srcname']
            parts = srcname.split('/')
            snet_sta = parts[0].split('_')
            snet = snet_sta[0]
            sta = snet_sta[1]
            latency = time() - s['slatest_time']
            alert, off_on = self.orbstat_alert_level(latency)
            source_dict[sta] = {}
            source_dict[sta]['latency'] = latency
            source_dict[sta]['latency_readable'] = self.humanize_time(latency)
            source_dict[sta]['snet'] = snet
            source_dict[sta]['alert'] = alert
            source_dict[sta]['offon'] = off_on
            source_dict[sta]['soldest_time'] = epoch2str(s['soldest_time'], "%Y-%m-%d %H:%M:%S")
            source_dict[sta]['slatest_time'] = epoch2str(s['slatest_time'], "%Y-%m-%d %H:%M:%S")
        return source_dict

    def orbstat_alert_level(self, secs):
        """Determine the alert level
        """
        if secs >= int(self.alerts['offline']):
            return 'down', 0
        elif secs >= int(self.alerts['warning']):
            return 'warning', 1
        else:
            return 'ok', 1

    def humanize_time(self, secs):
        """Create human readable
        timestamp
        """
        secs = round(secs)
        if secs < 60:
            return '%02ds' % (secs)
        else:
            mins,secs = divmod(secs,60)
            if mins < 60:
                return '%02dm:%02ds' % (mins, secs)
            else:
                hours,mins = divmod(mins,60)
                return '%02dh:%02dm:%02ds' % (hours, mins, secs)

    def add_orbstat(self, orbstat, sta, qtype=False):
        """Return station specific 
        orbstat values
        """
        orbstat_dict = {}
        if sta in orbstat:
            orbstat_dict['latency'] = orbstat[sta]['latency']
            orbstat_dict['latency_readable'] = self.humanize_time(orbstat[sta]['latency'])
            orbstat_dict['alert'] = orbstat[sta]['alert']
            orbstat_dict['status'] = orbstat[sta]['offon']
            if qtype == 'detail':
                orbstat_dict['slatest_time'] = orbstat[sta]['slatest_time']
                orbstat_dict['soldest_time'] = orbstat[sta]['soldest_time']
        else:
            orbstat_dict['latency'] = -1
            orbstat_dict['alert'] = 'down'
            orbstat_dict['status'] = 0
        return orbstat_dict
 
class NetworkSummary:
    """Methods for determining
    values of network and stations
    for JSON file(s)
    """
    def __init__(self, dbptr, pf, verbose=False, debug=False):
        """Initialize database pointers
        """
        self.dbptr = dbptr
        self.pf = pf
        self.verbose = verbose
        self.debug = debug

    def open_db(self):
        """Test that the database pointer
        is valid
        """
        try:
            db = datascope.dbopen(self.dbptr, 'r')
        except Exception,e:
            print "  - Cannot open database '%s'. Caught exception %s" % (self.dbptr, e)
            exit()
        else:
            self.db = db

    def close_db(self):
        """Close the open database 
        pointer. This will free() all
        dbpointers in memory
        """
        if self.db:
            db = self.db
            db.close()

    def idtag_determiner(self, dlogger_id):
        """The idtag field is only
        in dlsite, staq330 and q330comm
        tables. Dlsite is depreciated,
        q330comm first used 2006.
        Try q330comm first.
        """
        dbptr = datascope.dblookup(self.db, '', 'q330comm', '', '')
        dbptr.sort('ssident', unique=True)
        dbptr.subset('ssident=~/%s/' % dlogger_id)
        if dbptr.query('dbRECORD_COUNT') > 0:
            dbptr[3] = 0
            idtag = dbptr.getv('idtag')[0]
        else:
            idtag = 'N/A'
        dbptr.free()
        return idtag

    def create_instrument_history(self):
        """Open the snetsta table then
        join on stage, dlsensor and sitechan
        using very specific join keys (including
        time ranges). Then get histories for 
        all stations, returning dictionary
        """
        if self.verbose:
            print " - Working on create_instrument_history for all stations"

        # Get whole dlsensor table into simple per snident_dlident keyed list
        dlsensor_history = {}
        dlsensor_hist_dbptr = datascope.dblookup(self.dbptr, '', 'dlsensor', '', '') 
        for i in range(dlsensor_hist_dbptr.query('dbRECORD_COUNT')):
            dlsensor_hist_dbptr[3] = i 
            dlsen_dlmodel, dlsen_dlident, dlsen_chident, dlsen_time, dlsen_endtime, dlsen_snmodel, dlsen_snident = dlsensor_hist_dbptr.getv('dlmodel', 'dlident', 'chident', 'time', 'endtime', 'snmodel', 'snident')
            new_key = dlsen_snident + ' ' + dlsen_dlident
            dlsensor_history[new_key] = {'dlmodel':dlsen_dlmodel, 'dlident':dlsen_dlident, 'chident':dlsen_chident, 'time':dlsen_time, 'endtime':dlsen_endtime, 'snmodel':dlsen_snmodel, 'snident':dlsen_snident}
        # Get whole sitechan table into simple per station list
        instrument_history = {}
        sitechan_hist_dbptr = datascope.dblookup(self.dbptr, '', 'sitechan', '', '') 
        sitechan_hist_dbptr.subset('chan=~/^(BH|HN).*/')
        sitechan_hist_dbptr.sort(('sta', 'ondate', 'chan'))
        sitechan_hist_grp_dbptr = datascope.dbgroup(sitechan_hist_dbptr, 'sta')
        for i in range(sitechan_hist_grp_dbptr.query('dbRECORD_COUNT')):
            sitechan_hist_grp_dbptr[3] = i 
            sta, [db, view, end_rec, start_rec] = sitechan_hist_grp_dbptr.getv('sta', 'bundle')
            if self.verbose:
                print " - create_instrument_history(): Processing station %s" % sta
            instrument_history[sta] = []
            for j in range(start_rec, end_rec):
                sitechan_hist_dbptr[3] = j 
                site_chan, site_ondate, site_offdate, site_descrip, site_hang, site_vang = sitechan_hist_dbptr.getv('chan', 'ondate', 'offdate', 'descrip', 'hang', 'vang')
                if site_descrip != '' and len(site_descrip) > 1:
                    site_sensorid, site_dloggerid = site_descrip.split()
                    if site_descrip in dlsensor_history:

                        this_sensor = self.sensor_readable(dlsensor_history[site_descrip]['snmodel'])
                        this_datalogger = self.datalogger_readable(dlsensor_history[site_descrip]['dlmodel'])
                        datalogger_idtag = self.idtag_determiner(dlsensor_history[site_descrip]['dlident'])

                        sensor = {
                            'model':this_sensor[0], 
                            'css':this_sensor[1], 
                            'ssident':dlsensor_history[site_descrip]['snident']
                        }
                        datalogger = {
                            'model':this_datalogger[0], 
                            'css':this_datalogger[1], 
                            'ssident':dlsensor_history[site_descrip]['dlident'],
                            'idtag':datalogger_idtag
                        }

                        instrument_history[sta].append({
                            'channel':site_chan,
                            'ondate':site_ondate,
                            'offdate':site_offdate,
                            'snmodel':dlsensor_history[site_descrip]['snmodel'],
                            'sensor_id':dlsensor_history[site_descrip]['snident'],
                            'dlmodel':dlsensor_history[site_descrip]['dlmodel'],
                            'dlogger_id':dlsensor_history[site_descrip]['dlident'],
                            'snident':dlsensor_history[site_descrip]['snident'],
                            'chident':dlsensor_history[site_descrip]['chident'],
                            'hang':site_hang,
                            'vang':site_vang,
                            'datalogger':datalogger,
                            'sensor':sensor

                        })
                    else:
                        instrument_history[sta].append({
                            'channel':site_chan, 
                            'ondate':site_ondate, 
                            'offdate':site_offdate, 
                            'descrip':site_descrip, 
                            'sensor_id':site_sensorid, 
                            'dlogger_id':site_dloggerid,
                            'hang':site_hang,
                            'vang':site_vang,
                            'datalogger': {'model':'', 'css':'', 'ssident':'', 'idtag':''},
                            'sensor': {'model':'', 'css':'', 'ssident':''}
                        })
                else:
                    instrument_history[sta].append({
                        'channel':site_chan, 
                        'ondate':site_ondate, 
                        'offdate':site_offdate, 
                        'descrip':site_descrip,
                        'hang':site_hang,
                        'vang':site_vang,
                        'datalogger': {'model':'', 'css':'', 'ssident':'', 'idtag':''},
                        'sensor': {'model':'', 'css':'', 'ssident':''}
                    })
        sitechan_hist_grp_dbptr.free()
        sitechan_hist_dbptr.free()
        return instrument_history

    def create_deploy_pointer(self):
        """Create a pointer to the
        simple joined view of deployment
        """
        dbptr = datascope.dblookup(self.db, '', 'site', '', '')
        dbptr.join('snetsta')
        dbptr.join('deployment', outer=True)
        self.deploy_dbptr = dbptr

    def get_metadata(self):
        """Get all the meta data values for
        all fields of a database pointer, 
        including dbNULL, dbFIELD_DETAIL,
        dbFIELD_DESCRIPTION, dbFIELD_TYPE
        """
        if self.verbose:
            print " - Determining all null values, detail, descriptions and field types for schema"
        self.dbmeta = {}
        dbmeta = self.db
        for table in dbmeta.query('dbSCHEMA_TABLES'):
            dbmeta.lookup('', table, '', '')
            try:
                dbmeta.query('dbTABLE_FIELDS')
            except:
                pass
            else:
                for field in dbmeta.query('dbTABLE_FIELDS'):
                    if field not in self.dbmeta:
                        dbmeta.lookup('', '', field, 'dbNULL')
                        self.dbmeta[field] = {'null':'', 'detail':'', 'description':'', 'field_type':''}
                        try:
                            dbmeta.getv(field)
                        except:
                            pass
                        else: 
                            self.dbmeta[field]['null'] = dbmeta.getv(field)[0]
                            self.dbmeta[field]['detail'] = dbmeta.query('dbFIELD_DETAIL')
                            self.dbmeta[field]['description'] = dbmeta.query('dbFIELD_DESCRIPTION')
                            self.dbmeta[field]['field_type'] = dbmeta.query('dbFIELD_TYPE')
        datascope.dbfree(dbmeta)
        if debug:
            print "Metadata dictionary:\n"
            print self.dbmeta
        return self.dbmeta

    def field_definitions(self, dbptr, field):
        """Define what each field key 
        should be and what it should return
        """
        field_object = {}
        if field in self.dbmeta:
            detail = self.dbmeta[field]['detail']
            description = self.dbmeta[field]['description']
            field_type = self.dbmeta[field]['field_type']
            null = self.dbmeta[field]['null']
        value = dbptr.getv(field)[0]
        field_readable = '%s_readable' % field
        if value == null:
            if field == 'commtype':
                field_object['readable'] = 'unknown'
                field_object['css'] = 'unknown'
            elif field == 'provider':
                field_object['readable'] = 'unknown'
                field_object['css'] = 'unknown'
            else:
                field_object[field_readable] = "N/A"
        else:
            if field_type == datascope.dbREAL:
                field_object[field_readable] = round(value, 3)
            elif field_type == datascope.dbTIME:
                field_object[field_readable] = epoch2str( value, "%Y-%m-%d %H:%M:%S" )
            elif field_type == datascope.dbYEARDAY:
                field_object[field_readable] = epoch2str( epoch(value), "%Y-%m-%d" )
            else:
                field_object[field_readable] = value
            # Deal with specialized fields that have their own CSS
            if field == 'commtype':
                field_format = value.lower()
                field_format = field_format.replace(' ','_' )
                if field_format not in self.pf['comms']:
                    print "Comm type %s not in pf_comms_list list! Dbmaster error or new unaccounted for comm type!" % field_format
                    field_object['readable'] = 'unknown'
                    field_object['css'] = 'unknown'
                else:
                    pf_field_readable = self.pf['comms'][field_format]['name']
                    field_object['readable'] = pf_field_readable
                    field_object['css'] = field_format
            elif field == 'provider':
                field_format = value.lower()
                field_format = field_format.replace(' ','_' )
                field_format = field_format.replace('/','_' )
                field_format = field_format.strip("'")
                if field_format not in self.pf['provider']:
                    print "Provider type %s not in pf_provider_list list! Dbmaster error or new unaccounted for provider type!" % field_format
                    field_object['readable'] = 'unknown'
                    field_object['css'] = 'unknown'
                else:
                    pf_field_readable = self.pf['provider'][field_format]['name']
                    field_object['readable'] = pf_field_readable
                    field_object['css'] = field_format
        if debug:
            field_object['field'] = field
            field_object['detail'] = detail
            field_object['description'] = description
            field_object['type'] = field_type
            field_object['null'] = null
        field_object['value'] = value
        if debug:
            print field_object
        return field_object

    def process_decom_stations(self, field_list):
        """Process just decommissioned stations
        """
        if field_list == 'detail':
            fields = detail_dbmaster_fields
        elif field_list == 'summary':
            fields = decom_fields
        decom_dict = {}
        db_decom = datascope.dbsubset(self.deploy_dbptr, 'offdate != NULL || offdate < now()')
        db_decom.subset('endtime < now()')
        db_decom.join('comm')
        db_decom.join('sensor')
        db_decom.join('instrument')
        db_decom.join('adoption', pattern1=('sta'), outer=True)
        db_decom.subset('atype == NULL')
        db_decom.sort(['snet','sta'], unique=True)
        if verbose:
            print "  - Work through %d decommissioned stations" % db_decom.query('dbRECORD_COUNT')
        for i in range(db_decom.query('dbRECORD_COUNT')):
            db_decom[3] = i
            sta = db_decom.getv('sta')[0]
            decom_dict[sta] = {}
            for df in fields:
                df_obj = self.field_definitions(db_decom, df)
                df_readable_field = df + '_readable'
                if df == 'provider' or df == 'commtype':
                    decom_dict[sta][df] = {}
                    decom_dict[sta][df]['value'] = df_obj['readable']
                    decom_dict[sta][df]['css'] = df_obj['css']
                else:
                    decom_dict[sta][df] = df_obj[df_readable_field]
        datascope.dbfree(db_decom)
        return decom_dict

    def process_adopted_stations(self, field_list):
        """Process just adopted stations
        """
        if field_list == 'detail':
            fields = detail_dbmaster_adopt_fields
        elif field_list == 'summary':
            fields = adopt_fields
        adopt_dict = {}
        db_adopt = datascope.dbsubset(self.deploy_dbptr, 'offdate != NULL || offdate < now()')
        db_adopt.subset('endtime < now()')
        db_adopt.join('comm')
        db_adopt.join('sensor')
        db_adopt.join('instrument')
        db_adopt.sort(['snet', 'sta'])
        db_adopt.join('adoption', pattern1=('sta'), outer=True)
        db_adopt.subset('atype != NULL')
        db_adopt.sort(['snet', 'sta'])
        db_adopt.group('sta')
        if verbose:
            print "  - Work through %d adopted stations" % db_adopt.query('dbRECORD_COUNT')
        for i in range(db_adopt.query('dbRECORD_COUNT')):
            db_adopt[3] = i
            sta = db_adopt.getv('sta')[0]
            dbptr_sub = datascope.dbsubset(db_adopt, 'sta =~ /%s/' % sta)
            dbptr_sub.ungroup()
            dbptr_sub.sort(["adoption.time"], reverse=True)
            dbptr_sub[3] = 0 # Only get the most recent deployment
            sta = dbptr_sub.getv('sta')[0]
            adopt_dict[sta] = {}
            for adf in fields:
                adf_obj = self.field_definitions(dbptr_sub, adf)
                adf_readable_field = adf + '_readable'
                if( adf == 'provider' or adf == 'commtype' ):
                    adopt_dict[sta][adf] = {}
                    adopt_dict[sta][adf]['value'] = adf_obj['readable']
                    adopt_dict[sta][adf]['css'] = adf_obj['css']
                else:
                    adopt_dict[sta][adf] = adf_obj[adf_readable_field]
        datascope.dbfree(db_adopt)
        return adopt_dict

    def process_active_stations(self, field_list):
        """Process currently active
        stations
        """
        if field_list == 'detail':
            fields = detail_dbmaster_fields
        elif field_list == 'summary':
            fields = active_fields
        active_dict = {}
        db_active = datascope.dbsubset(self.deploy_dbptr, 'offdate == NULL || offdate >= now()')
        db_active.subset('offdate == NULL || offdate >= now()')
        db_active.subset('endtime >= now()')
        db_active.subset('time <= now()')
        db_active.join('comm', outer=True)
        db_active.subset('comm.endtime == NULL || comm.endtime >= now()')
        db_active.join('sensor', outer=True)
        db_active.join('instrument', outer=True)
        db_active.subset('sensor.endtime == NULL || sensor.endtime >= now()')
        db_active.subset('insname != NULL')
        db_active.subset('chan =~ /BHZ.*/') # Include loc_codes
        db_active.subset('samprate >= 40')
        db_active.sort(("sta"), unique=True)
        if verbose:
            print "  - Work through %d active stations" % db_active.query('dbRECORD_COUNT')
        for i in range(db_active.query('dbRECORD_COUNT')):
            db_active[3] = i
            sta = db_active.getv('sta')[0]
            active_dict[sta] = {}
            for af in fields:
                af_obj = self.field_definitions(db_active, af)
                af_readable_field = af + '_readable'
                if af == 'provider' or af == 'commtype':
                    active_dict[sta][af] = {}
                    active_dict[sta][af]['value'] = af_obj['readable']
                    active_dict[sta][af]['css'] = af_obj['css']
                else:
                    active_dict[sta][af] = af_obj[af_readable_field]
        datascope.dbfree(db_active)
        return active_dict
#
#    def instrument_history(self, sta, intype='summary'):
#        """Return dictionary of instrument history
#        Have to use BHN or BNE (not BHZ) for sensor history
#        as sensor resets are typically E-W or N-S
#        This is a little more complex than the usual grouped
#        records, so cannot use utility function process_grouped_records()
#        """
#        inst_list = []
#        inst_hist = datascope.dbsubset(self.instrument_history_dbptr, 'sta =~ /%s/' % sta)
#        inst_hist.sort('ondate')
#        for i in range(inst_hist.query('dbRECORD_COUNT')):
#            inst_hist[3] = i
#            snet, sta, chan, time, ssident, dlmodel, snmodel, snident, ondate, offdate, hang, vang, descrip = inst_hist.getv('snet', 'sta', 'chan', 'time', 'ssident', 'dlmodel', 'snmodel', 'snident', 'ondate', 'offdate', 'hang', 'vang', 'descrip' )
#            this_sensor = self.sensor_readable(snmodel)
#            this_datalogger = self.datalogger_readable(dlmodel)
#            datalogger_idtag = self.idtag_determiner(ssident)
#            sensor = {'model':this_sensor[0], 'css':this_sensor[1], 'ssident':snident}
#            datalogger = { 'model':this_datalogger[0], 'css':this_datalogger[1], 'ssident':ssident, 'idtag':datalogger_idtag}
#            inst_list.append({'channel':chan, 'hang':hang, 'vang':vang, 'sensor':sensor, 'datalogger':datalogger, 'ondate':ondate, 'offdate':offdate})
#        datascope.dbfree(inst_hist)
#        if intype == 'summary':
#            return inst_list.pop()
#        elif intype == 'detail':
#            return inst_list

    def baler_history(self):
        """Retrieve and return baler history 
        depending on query type
        """
        if self.verbose:
            print " - Working on baler_history for all stations"
        baler_dict = {}
        stabaler_ptr = datascope.dblookup(self.db, '', 'stabaler', '', '')
        stabaler_ptr.sort(('dlsta','time'))
        try:
            baler_dict = self.process_grouped_records(stabaler_ptr, 'dlsta', detail_baler_fields)
        except LookupError,e:
            print 'baler_history(): LookupError: %s' % (e)
        stabaler_ptr.free()
        return baler_dict

    def process_grouped_records(self, dbpt, dict_key, my_fields):
        """Process groups of records
        """
        my_dict = {}
        my_group = datascope.dbgroup(dbpt, dict_key)
        if my_group.query('dbRECORD_COUNT') > 0:
            for i in range(my_group.query('dbRECORD_COUNT')):
                my_group[3] = i
                my_dict_key, [db, view, end_rec, start_rec] = my_group.getv(dict_key, 'bundle')
                # my_bundle is a list describing a view where
                # [ db, view, end_rec, start_rec ]
                # Make a list - simplest to sort
                # Generate all keys for the dictionary
                my_dict[my_dict_key] = []
                for j in range(start_rec, end_rec):
                    dbpt[3] = j
                    my_sub_dict = {}
                    for f in my_fields:
                        my_f_obj = self.field_definitions(dbpt, f)
                        readable_field = '%s_readable' % f
                        my_sub_dict[f] = my_f_obj[readable_field]
                    my_dict[my_dict_key].append(my_sub_dict)
            return my_dict
        else:
            raise LookupError('No groupable records for this station')

    def infrasound_sensors(self, imap):
        """Determine what infrasound sensors are installed for both
        summary JSON and detailed station JSON files
        Kind of a mess right now due to how the channels map to one
        or more sensors, so we have to have a 'holder' list 
        and then get unique values of that list to append and return
        This is a little more complex than the usual grouped
        records, so cannot use utility function process_grouped_records()
        """
        qstr = '|'.join([ '|'.join(v) for k,v in imap.iteritems()]) # Build the Datascope query str
        if self.verbose:
            print "  - infrasound_sensors(): Searching sitechan table for chans that match: %s" % qstr
        infrasound_history = {}
        infra_hist_dbptr = datascope.dblookup(self.db, '', 'sitechan', '', '')
        try:
            infra_hist_dbptr.subset('chan=~/(%s)/' % qstr)
        except Exception,e:
            print "  - infrasound_sensors(): Error: subset on sitechan table failed with exception: %s" % e
        else:
            if debug:
                print "  - infrasound_sensors(): There are %d records" % infra_hist_dbptr.query('dbRECORD_COUNT')
            infra_hist_dbptr.sort(('sta', 'ondate', 'chan'))
            infra_hist_grp_dbptr = datascope.dbgroup(infra_hist_dbptr, 'sta')
            for i in range(infra_hist_grp_dbptr.query('dbRECORD_COUNT')):
                infra_hist_grp_dbptr[3] = i
                sta, [db, view, end_rec, start_rec] = infra_hist_grp_dbptr.getv('sta', 'bundle')
                # Generate all keys for the dictionary
                if self.verbose:
                    print "  - infrasound_sensors(): Processing station %s" % sta
                infrasound_history[sta] = {'current':[], 'history':{}}
                infrachans_history_holder = []
                # Process all records per station
                for j in range(start_rec, end_rec):
                    infra_hist_dbptr[3] = j
                    my_sub_dict = {}
                    ondate, offdate, chan = infra_hist_dbptr.getv('ondate', 'offdate', 'chan')
                    my_sub_dict['ondate'] = ondate
                    my_sub_dict['chan'] = chan
                    for sentype, senchans in imap.iteritems():
                        if chan in senchans:
                            infra_sensor = sentype
                    my_sub_dict['sensor'] = infra_sensor
                    if self.verbose and (sta == '442A' or sta == '214A' or sta == 'MDND'):
                        print infrasound_history[sta]['current']
                    if offdate == self.dbmeta['offdate']['null']:
                        offdate = 'N/A'
                        if not infra_sensor in infrasound_history[sta]['current']:
                            infrasound_history[sta]['current'].append(infra_sensor)
                    else:
                        if infra_sensor in infrasound_history[sta]['current']:
                            infrasound_history[sta]['current'].remove(infra_sensor)
                    my_sub_dict['offdate'] = offdate
                    infrachans_history_holder.append(my_sub_dict)

                infrasound_history[sta]['history'] = infrachans_history_holder

                if ('MEMS' in infrasound_history[sta]['current']) and ('SETRA' in infrasound_history[sta]['current']) and ('NCPA' in infrasound_history[sta]['current']):
                    infrasound_history[sta]['current'] = 'complete'
                elif 'MEMS' in infrasound_history[sta]['current']:
                    infrasound_history[sta]['current'] = 'mems'
                else:
                    infrasound_history[sta]['current'] = 'none'
            infra_hist_grp_dbptr.free()
            infra_hist_dbptr.free()
            return infrasound_history

    def deployment_history(self):
        """Return the deployment
        history as a dictionary
        """
        if self.verbose:
            print "  - Working on deployment_history for all stations"
        deploy_hist_dict = {}
        deploy_ptr = datascope.dbsort(self.deploy_dbptr, ('sta', 'time'))
        deploy_grp_ptr = datascope.dbgroup(deploy_ptr, 'sta')
        for i in range(deploy_grp_ptr.query('dbRECORD_COUNT')):
            deploy_grp_ptr[3] = i
            sta, [db, view, end_rec, start_rec] = deploy_grp_ptr.getv('sta', 'bundle')
            # Only create dictionary entry if more than one deployment
            if (end_rec - start_rec) > 1:
                deploy_hist_dict[sta] = []
                for j in range(start_rec, end_rec):
                    per_deploy_dict = {}
                    deploy_ptr[3] = j
                    for ddhf in detail_deployment_hist_fields:
                        ddhf_obj = self.field_definitions(deploy_ptr, ddhf)
                        ddhf_readable_field = '%s_readable' % ddhf
                        per_deploy_dict[ddhf] = ddhf_obj[ddhf_readable_field]
                    deploy_hist_dict[sta].append(per_deploy_dict)
        deploy_grp_ptr.free()
        deploy_ptr.free()
        return deploy_hist_dict

    def comms_history(self):
        """Return the communications
        history as a dictionary
        """
        if self.verbose:
            print "  - Working on comms_history for all stations"
        comms_dict = {}
        comms_ptr = datascope.dblookup(self.db, '', 'comm', '', '')
        comms_ptr.sort(('sta','time'))
        comms_grp_ptr = datascope.dbgroup(comms_ptr, 'sta')
        for i in range(comms_grp_ptr.query('dbRECORD_COUNT')):
            comms_grp_ptr[3] = i
            sta, [db, view, end_rec, start_rec] = comms_grp_ptr.getv('sta', 'bundle')
            comms_dict[sta] = []
            per_sta_comms_dict = {}
            for j in range(start_rec, end_rec):
                comms_ptr[3] = j
                for dchf in detail_comms_hist_fields:
                    dchf_obj = self.field_definitions(comms_ptr, dchf)
                    dchf_readable_field = '%s_readable' % dchf
                    if dchf == 'provider' or dchf == 'commtype':
                        per_sta_comms_dict[dchf] = {}
                        per_sta_comms_dict[dchf]['value'] = dchf_obj['readable']
                        per_sta_comms_dict[dchf]['css'] = dchf_obj['css']
                    else:
                        per_sta_comms_dict[dchf] = dchf_obj[dchf_readable_field]
                comms_dict[sta].append(per_sta_comms_dict)
        comms_grp_ptr.free()
        comms_ptr.free()
        return comms_dict

    def calibration_history(self, dbpath):
        """Return calibration history
        This is a little more complex than the usual grouped
        records, so cannot use utility function process_grouped_records()
        """
        if self.verbose:
            print " - Working on calibration_history for all stations"
        calibration_dict = {}
        calib_ptr = datascope.dbopen(dbpath, 'r')
        calib_ptr.lookup(table='calplot')
        calib_ptr.sort(('sta', 'time'))
        calib_grp_ptr = datascope.dbgroup(calib_ptr, 'sta')
        if self.debug:
            print "  - calibration_history(): There are %d grouped records" % calib_grp_ptr.query('dbRECORD_COUNT')
        for i in range(calib_grp_ptr.query('dbRECORD_COUNT')):
            calib_grp_ptr[3] = i
            sta, [db, view, end_rec, start_rec] = calib_grp_ptr.getv('sta', 'bundle')
            if self.debug:
                print "   - calibration_history(): Processing station %s" % sta
            calibration_dict[sta] = []
            # Temporaray holder dictionary
            calib_holder = {}
            for j in range(start_rec, end_rec):
                calib_ptr[3] = j
                chan, time = calib_ptr.getv('chan', 'time')
                time_int = int(time)
                if not time_int in calib_holder:
                    calib_holder[time_int] = []
                calib_holder[time_int].append({'chan':chan, 'file':calib_ptr.extfile()})
            # Sort the dictionary and append to list
            for key in sorted(calib_holder.iterkeys()):
                calibration_dict[sta].append({'time':key,'chanfiles':calib_holder[key]})
        calib_grp_ptr.free()
        calib_ptr.close()
        return calibration_dict

    def dlevents_history(self, dbpath):
        """Return all datalogger
        events for all stations
        as a dictionary
        """
        if self.verbose:
            print " - Working on dlevent_history for all stations"
        dlevents_dict = {}
        dlevs_ptr = datascope.dbopen(dbpath, 'r')
        dlevs_ptr.lookup(table='dlevent')
        dlevs_ptr.sort(('dlname','time'))
        dlevs_grp_ptr = datascope.dbgroup(dlevs_ptr, 'dlname')
        for i in range(dlevs_grp_ptr.query('dbRECORD_COUNT')):
            dlevs_grp_ptr[3] = i
            dlname, [db, view, end_rec, start_rec] = dlevs_grp_ptr.getv('dlname', 'bundle')
            dlevents_dict[dlname] = {}
            for j in range(start_rec, end_rec):
                dlevs_ptr[3] = j
                time = dlevs_ptr.getv('time')[0]
                time_int = int(time)
                year, month = epoch2str(time,"%Y_%L").split('_')
                month = month.strip()
                if year not in dlevents_dict[dlname]:
                    dlevents_dict[dlname][year] = {}
                if month not in dlevents_dict[dlname][year]:
                    dlevents_dict[dlname][year][month] = {}
                dlevents_dict[dlname][year][month][time_int] = {}
                for ddle in detail_dlevents:
                    my_ddle_obj = self.field_definitions(dlevs_ptr, ddle)
                    readable_field = '%s_readable' % ddle
                    dlevents_dict[dlname][year][month][time_int][ddle] = my_ddle_obj[readable_field]
        dlevs_grp_ptr.free()
        dlevs_ptr.close()
        return dlevents_dict

    def sensor_readable(self, insname):
        """Use pf values to determine sensor values
        Force match to be a string in case of just int
        values in the regex
        """
        smodel = False
        sclass = False
        l_insname = insname.lower()
        for k in self.pf['sensors']:
            for match in self.pf['sensors'][k]['regex']:
                if str(match) in l_insname:
                    smodel = self.pf['sensors'][k]['name']
                    sclass = k
        if not smodel:
            print "sensor_readable(): Error: instrument %s is not one of the options!" % insname
            smodel = 'unknown'
        if not sclass:
            sclass = 'unknown'
        return smodel, sclass

    def datalogger_readable(self, insname):
        """Use pf values to determine datalogger values
        Force match to be a string in case of just int
        values in the regex
        """
        dmodel = False
        dclass = False
        l_insname = insname.lower()
        for k in self.pf['dataloggers']:
            for match in self.pf['dataloggers'][k]['regex']:
                if str(match) in l_insname:
                    dmodel = self.pf['dataloggers'][k]['name']
                    dclass = k
        if not dmodel:
            print "datalogger_readable(): Error: instrument %s is not one of the options!" % insname
            dmodel = 'unknown'
        if not dclass:
            dclass = 'unknown'
        return dmodel, dclass

def main():
    """Main processing script
    for all JSON summary & individual
    files
    """
    print "Start of script at time %s" % strftime("%a, %d %b %Y %H:%M:%S +0000", gmtime())
    if debug:
        print " - DEBUGGING ON!"
    if verbose:
        print " - Parse stations.pf"
    stations_pf = 'stations.pf'
    pfupdate(stations_pf)
    stapf = {}
    stapf['network'] = pfget(stations_pf, 'network')
    stapf['provider'] = pfget(stations_pf, 'comms_provider')
    stapf['comms'] = pfget(stations_pf, 'comms_type')
    stapf['dataloggers'] = pfget(stations_pf, 'datalogger')
    stapf['sensors'] = pfget(stations_pf, 'sensor')
    common_pf = 'common.pf'
    if verbose:
        print " - Parse configuration parameter file (%s)" % common_pf
    pfupdate(common_pf)
    json_path = '%s/stations' % pfget(common_pf, 'CACHEJSON')
    all_stations_json_file = '%s/stations_beta.json' % json_path
    dbmaster = pfget(common_pf, 'USARRAY_DBMASTER')
    q330comms = pfget(common_pf, 'USARRAY_Q330COMMS')
    usarray_orb = pfget(common_pf, 'USARRAY_ORB')
    usarray_orb_stations_select  = pfget(common_pf, 'USARRAY_ORB_STATIONS_SELECT')
    cascadia_orb = pfget(common_pf, 'CASCADIA_ORB')
    cascadia_orb_stations_select = pfget(common_pf, 'CASCADIA_ORB_STATIONS_SELECT')
    orbstat_alerts = pfget(common_pf, 'ORBSTAT_ALERTS')
    infrasound_mapping = pfget(common_pf, 'INFRASOUND_MAPPING')
    if verbose:
        print "  - Infrasound mapping:"
        print infrasound_mapping
    calibrations = pfget(common_pf, 'DBCALIB')
    dbops_q330 = pfget(common_pf, 'DBOPS_Q330')
    landowner_dir = "%s_local" % pfget(common_pf, 'CACHE_REPORTS_LANDOWNER')
    station_digest_dir = "%s_local" % pfget(common_pf, 'CACHE_REPORTS_STATION_DIGEST')
    if verbose:
        print "  - USArray dbmaster path is %s\n  - USArray orb path is %s\n  - USArray orbselect statement is %s" % (dbmaster, usarray_orb, usarray_orb_stations_select)

    nfs_test(dbmaster)

    myorb = ParseOrb(orbstat_alerts)
    orbstatus = {} 
    orbstatus.update(myorb.orb_interaction(usarray_orb, usarray_orb_stations_select))
    orbstatus.update(myorb.orb_interaction(cascadia_orb, cascadia_orb_stations_select))

    if verbose:
        db = NetworkSummary(dbmaster, stapf, verbose=True)
    else:
        db = NetworkSummary(dbmaster, stapf)

    print "- Summary JSON file processing"
    db.open_db()
    db.get_metadata()
    # metadata = db.get_metadata()
    db.create_deploy_pointer()
    station_dict['decom'] = db.process_decom_stations('summary')
    station_dict['adopt'] = db.process_adopted_stations('summary')
    station_dict['active'] = db.process_active_stations('summary')
    instrument_history = db.create_instrument_history()
    baler_history = db.baler_history()
    infrasound_history = db.infrasound_sensors(infrasound_mapping)

    print " - Decom stations: Add most recent instrument & baler history"
    for sta in sorted(station_dict['decom'].iterkeys()):
        dlsta = '%s_%s' % (station_dict['decom'][sta]['snet'], sta)
        if verbose:
            print "  - Processing decom station: %s" % sta
        if station_dict['decom'][sta]['snet'] == 'TA':
            try:
                recent_inst_hist = instrument_history[sta][-1]
            except LookupError,e:
                print "instrument_history(): LookupError: %s" % e
            else:
                station_dict['decom'][sta]['sensor'] = {}
                station_dict['decom'][sta]['datalogger'] = {}
                station_dict['decom'][sta]['sensor']['value'] = recent_inst_hist['sensor']['model']
                station_dict['decom'][sta]['sensor']['css'] = recent_inst_hist['sensor']['css']
                station_dict['decom'][sta]['datalogger']['value'] = recent_inst_hist['datalogger']['model']
                station_dict['decom'][sta]['datalogger']['css'] = recent_inst_hist['datalogger']['css']
            if dlsta in baler_history:
                station_dict['decom'][sta]['baler'] = baler_history[dlsta][-1]
            # Need this for most recent sensor in top right
            if sta in infrasound_history:
                if 'current' in infrasound_history[sta]:
                    station_dict['decom'][sta]['infrasound'] = infrasound_history[sta]['current']
                else:
                    station_dict['decom'][sta]['infrasound'] = infrasound_history[sta]['history'][-1]
            else:
                station_dict['decom'][sta]['infrasound'] = 'unknown'


    print " - Active stations: Add most recent instrument & baler history"
    for sta in sorted(station_dict['active'].iterkeys()):
        dlsta = '%s_%s' % (station_dict['active'][sta]['snet'], sta)
        if verbose:
            print "  - Processing active station: %s" % sta
        station_dict['active'][sta]['orbstat'] = myorb.add_orbstat(orbstatus, sta)
        if station_dict['active'][sta]['snet'] == 'TA':
            try:
                recent_inst_hist = instrument_history[sta][-1]
            except LookupError,e:
                print "instrument_history(): LookupError: %s" % e
            else:
                station_dict['active'][sta]['sensor'] = {}
                station_dict['active'][sta]['datalogger'] = {}
                station_dict['active'][sta]['sensor']['value'] = recent_inst_hist['sensor']['model']
                station_dict['active'][sta]['sensor']['css'] = recent_inst_hist['sensor']['css']
                station_dict['active'][sta]['datalogger']['value'] = recent_inst_hist['datalogger']['model']
                station_dict['active'][sta]['datalogger']['css'] = recent_inst_hist['datalogger']['css']
            if sta in infrasound_history:
                station_dict['active'][sta]['infrasound'] = infrasound_history[sta]['current']
            else:
                station_dict['active'][sta]['infrasound'] = 'unknown'
            if dlsta in baler_history:
                station_dict['active'][sta]['baler'] = baler_history[dlsta][-1]
        else:
            station_dict['active'][sta]['infrasound'] = 'unknown'
            station_dict['active'][sta]['baler'] = 'unknown'
    db.close_db()

    if verbose:
        print " - Dump summary JSON file for all stations"
    f = open(all_stations_json_file+'+', 'w') 
    json.dump(station_dict, f, sort_keys=True, indent=2)
    f.flush()

    try:
        os.rename(all_stations_json_file+'+', all_stations_json_file)
    except OSError:
        print "  - Cannot rename summray JSON file. Permissions problem?"

    if zipper:
        if verbose:
            print "- Create gzip file: %s.gz" % all_stations_json_file
        zip_me_up(all_stations_json_file)

    print "- Start to create individual station JSON files at %s" % strftime("%a, %d %b %Y %H:%M:%S +0000", gmtime())
    # Create list of the station types to process in detail
    if not subtype or subtype == 'all':
        my_station_types = ['decom', 'adopt', 'active']
    else:
        my_station_types = [subtype]
    persta = NetworkSummary(dbmaster, stapf)
    persta.open_db()
    persta.get_metadata()
    persta.create_deploy_pointer()
    # instrument_history = persta.create_instrument_history()
    # baler_history = persta.baler_history()
    # infrasound_history = persta.infrasound_sensors(infrasound_mapping)
    calibration_history = persta.calibration_history(calibrations)
    dlevents_history = persta.dlevents_history(dbops_q330)
    comms_history = persta.comms_history()
    deployment_history = persta.deployment_history()

    # Process each station type
    for sta_type in my_station_types:
        print "  - Start working on %s stations at %s" % (sta_type, strftime("%a, %d %b %Y %H:%M:%S +0000", gmtime()))
        if sta_type == 'decom':
            sta_type_dict = persta.process_decom_stations('detail')
        elif sta_type == 'adopt':
            sta_type_dict = persta.process_adopted_stations('detail')
        elif sta_type == 'active':
            sta_type_dict = persta.process_active_stations('detail')

        for sta in sorted(sta_type_dict.iterkeys()):
            dlsta = '%s_%s' % (sta_type_dict[sta]['snet'], sta)
            sta_dict = {
                "baler_history": {}, 
                "calibration_history": {}, 
                "comms_history": {}, 
                "dlevents": {}, 
                "infrasound_history": {},
                "instrument_history": {}, 
                "metadata": {}
            }
            if verbose:
                print "  - Processing station detail for %s" % sta
                print "   - Map stype_dict[%s] to this_sta_dict['metadata']" % sta
            sta_dict['metadata'] = sta_type_dict[sta]

            if verbose:
                print "   - Current instrumentation"
            try:
                recent_inst_hist = instrument_history[sta][-1]
            except LookupError,e:
                print "instrument_history(): LookupError: %s" % e
            else:
                sta_dict['metadata']['sensor'] = {}
                sta_dict['metadata']['datalogger'] = {}
                sta_dict['metadata']['sensor']['value'] = recent_inst_hist['sensor']['model']
                sta_dict['metadata']['sensor']['css'] = recent_inst_hist['sensor']['css']
                sta_dict['metadata']['datalogger']['value'] = recent_inst_hist['datalogger']['model']
                sta_dict['metadata']['datalogger']['css'] = recent_inst_hist['datalogger']['css']

            if sta_dict['metadata']['snet'] == 'TA':
                if sta in infrasound_history:
                    sta_dict['infrasound_history'] = infrasound_history[sta]['history']
                if dlsta in baler_history:
                    sta_dict['baler_history'] = baler_history[dlsta]
                if sta in instrument_history:
                    sta_dict['instrument_history'] = instrument_history[sta]
                if sta in calibration_history:
                    sta_dict['calibration_history'] = calibration_history[sta]
                if dlsta in dlevents_history:
                    sta_dict['dlevents'] = dlevents_history[dlsta]

            if sta in comms_history:
                sta_dict['comms_history'] = comms_history[sta]

            if sta in deployment_history:
                sta_dict['deployment_history'] = deployment_history[sta]

            if verbose:
                print "   - Per station JSON file..."
            sta_file = '%s/%s_%s.json' % (json_path, sta_dict['metadata']['snet'], sta)
            sta_file_pre = '%s+' % sta_file
            fs = open(sta_file_pre, 'w') 
            json.dump(sta_dict, fs, sort_keys=True, indent=2)
            fs.flush()
            try:
                os.rename(sta_file_pre, sta_file)
            except OSError,e:
                print "- Renaming JSON file to %s failed" % sta_file

    persta.close_db()
    print "End of script at time %s" % strftime("%a, %d %b %Y %H:%M:%S +0000", gmtime())
    return 0

if __name__ == '__main__':
    status = main()
    sys.exit(status)
